import numpy as np

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        #è¿™ä¸ªåˆ›å»ºçš„æ˜¯ä¸€ä¸ªç©ºå­—å…¸
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
    #è¡¥å……ä¸¤ä¸ªæ¿€æ´»å‡½æ•°çš„ä»£ç 

    def sigmoid(self,a):
        return 1 / (1 + np.exp(-a))

    #è¿™é‡Œè·Ÿç»´åº¦æ²¡å…³ç³»å—
    #è¿™é‡Œç›¸å½“äºå¯¹æ¯ä¸€ä¸ªå¯èƒ½æ€§éƒ½åšäº†softmax
    def softmax(self,a):
        #a = np.array([[1,2,3],[4,5,6],[7,8,9]])
        if a.ndim == 2:
            #åœ¨åŒä¸€è¡Œçš„ä¸åŒåˆ—ä¸Šå–æœ€å¤§å€¼
            c = np.max(a,axis = 1, keepdims = True)
            #è¿™é‡Œç›¸å½“äºå¯¹æ¯ä¸€ä¸ªå…ƒç´ éƒ½åšäº†eçš„è¿™ä¸ªå…ƒç´ æ¬¡æ–¹
            exp_a = np.exp(a - c)
            sum_exp_a = np.sum(exp_a, axis = 1, keepdims = True)
            return exp_a / sum_exp_a
        else:
            c = np.max(a)
            exp_a = np.exp(a - c)
            return exp_a / np.sum(exp_a)


    #å‰å‘ä¼ æ’­çš„ä»£ç 
    def predict(self, X):
        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']
        a1 = np.dot(X, W1) + b1
        z1 = self.sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = self.softmax(a2)
        return y
    #å¾—æœ‰ä¸€ä¸ªè®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°çš„
    def cross_entropy_error(self,y, t):
        delta = 1e-7
        if y.ndim == 1:
            #å¦‚æœæ˜¯å•æ ·æœ¬çš„æƒ…å†µå°±æŠŠä»–ä»¬ğŸ™†å˜æˆå¤šç»´åº¦çš„ï¼Œæˆ‘ç†è§£ æ¯”å¦‚y =[1,2,3]ç„¶å å°±å˜æˆ1è¡Œä¸‰åˆ—[[1,2,3]]è¿™æ ·çš„ï¼Ÿ
            #ä¿è¯â€œæ‰¹æ¬¡ç»´â€å­˜åœ¨ï¼ˆæ–¹ä¾¿ç»Ÿä¸€çŸ©é˜µè¿ç®—ï¼‰
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)#yä¹ŸåŒç†
        batch_size = y.shape[0]
        #èŠ±å¼ç´¢å¼• ç›´æ¥å–å‡ºæ¥æ¯è¡Œçš„å¯¹åº”æ ‡ç­¾å¤„çš„å…ƒç´  è¿™é‡Œç›¸å½“äºå¯¹äºone hotç¼–ç åšäº†é€‚åº”
        #æœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„æ— æ‰€è°“
        return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size
    #å‰å‘ä¼ æ’­å®Œäº†å¾—æ±‚å‡†ç¡®ç‡å§
    def accuracy(self, X, t):
        y = self.predict(X)
        #np.argmax(y.axis =1) è¿”å›çš„æ˜¯ æœ€å¤§å€¼çš„ç´¢å¼•
        #å’Œå¯¹åº”æ ‡ç­¾èƒ½å¯¹å¾—ä¸Šçš„ åŠ èµ·æ¥ å°±æ˜¯èƒ½åŒ¹é…ä¸Šçš„æ•°é‡äº†ï¼Œ/æ€»æ•° ä¹Ÿå°±æ˜¯æ€»çš„æ‰¹æ¬¡ï¼Œæ€»çš„ä¸ªæ•°
        acc = np.sum(np.argmax(y, axis = 1) == np.argmax(t, axis = 1)) / y.shape[0]
        return acc
    def loss(self, X, t):
        y = self.predict(X)
        loss = self.cross_entropy_error(y, t)
        return loss

    def numerical_gradient(self,f, x):
        h = 1e-4
        grad = np.zeros_like(x)  # ç”Ÿæˆå’Œxå½¢çŠ¶ç›¸åŒçš„æ•°ç»„

        for idx in range(x.size):
            tmp_val = x[idx]
            x[idx] = tmp_val + h
            fxh1 = f(x)

            x[idx] = tmp_val - h
            fxh2 = f(x)
            grad[idx] = (fxh1 - fxh2) / (2 * h)
            x[idx] = tmp_val
        return grad

    def numerical_gradient_all(self, x, t):
        loss_w =lambda W:self.loss(x, t)
        grads = {}
        grads['W1'] = self.numerical_gradient(loss_w, self.params['W1'])
        grads['b1'] = self.numerical_gradient(loss_w, self.params['b1'])
        grads['W2'] = self.numerical_gradient(loss_w, self.params['W2'])
        grads['b2'] = self.numerical_gradient(loss_w, self.params['b2'])
        return grads
#3.6.2ç¥ç»ç½‘ç»œçš„æ¨ç†å¤„ç†
#æ¥ä¸‹æ¥å¯¹è¿™ä¸ªæ•°æ®é›†å®ç°ç¥ç»ç½‘ç»œçš„æ¨ç†å¤„ç†ï¼Œè¾“å…¥å±‚æœ‰784ä¸ªç¥ç»å…ƒ è¾“å‡ºå±‚æœ‰10ä¸ªç¥ç»å…ƒ
#ä¸¤ä¸ªéšè—å±‚ ä¸€å±‚100ä¸ª ä¸€å±‚50 ä¸ª
import numpy as np
import struct

def load_images(path):
    with open(path, 'rb') as f:
        magic, num, rows, cols = struct.unpack(">IIII", f.read(16))
        data = np.frombuffer(f.read(), dtype=np.uint8)
        return data.reshape(num, rows, cols)

def load_labels(path):
    with open(path, 'rb') as f:
        magic, num = struct.unpack(">II", f.read(8))
        labels = np.frombuffer(f.read(), dtype=np.uint8)
        return labels




if __name__ == '__main__':
    #åŠ è½½æ•°æ®
    train_images = load_images("../3/data/train-images-idx3-ubyte")
    train_labels = load_labels("../3/data/train-labels-idx1-ubyte")
    test_images = load_images("../3/data/t10k-images-idx3-ubyte")
    test_labels = load_labels("../3/data/t10k-labels-idx1-ubyte")

    print(train_images.shape, train_labels.shape)
    print(test_images.shape, test_labels.shape)
    train_images_flat = train_images.reshape(train_images.shape[0], -1)
    print(train_images_flat.shape, train_labels.shape)
    test_images_flat = test_images.reshape(test_images.shape[0], -1)
    print(test_images_flat.shape, test_labels.shape)

    train_loss_list = []
    #è¶…å‚æ•°
    iters_num = 10000
    train_size = train_images_flat.shape[0]
    batch_size = 100
    learning_rate = 0.1
    net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)
    for i in range(iters_num):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = train_images_flat[batch_mask]
        t_batch = train_labels[batch_mask]

        #è®¡ç®—æ¢¯åº¦
        grad = net.numerical_gradient_all(x_batch, t_batch)

        #æ›´æ–°å‚æ•°
        for key in net.params.keys():
            net.params[key] -= learning_rate * grad[key]
        loss = net.loss(x_batch, t_batch)
        train_loss_list.append(loss)